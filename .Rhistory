posteriorweight[kminus+1,l] <-  log(alpha/ (N-1+alpha)) +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+1],1:D], Q= S[active[kminus+1],1:D,1:D], log = TRUE)
}
res2 <- try(dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+2],1:D], Q= S[active[kminus+2],1:D,1:D]), silent=TRUE)
if (class(res) == "try-error"){
posteriorweight[kminus+2,l] <- -Inf
} else{
posteriorweight[kminus+2,l] <-  log(alpha/ (N-1+alpha))  +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+2],1:D], Q= S[active[kminus+2],1:D,1:D], log = TRUE)
}
weights[,l] <- exp(posteriorweight[,l])/sum(exp(posteriorweight[,l]))
if (sum(exp(posteriorweight[,l])) < 1e-200){
ctemp.new[l] <- sample(active, 1, prob = rep(1,length(active)), replace = TRUE)
} else {
ctemp.new[l] <- sample(active, 1, prob= weights[,l], replace = TRUE)
}
modelweights[count] <- sum(exp((1/N.new) *apply(posteriorweight,1,sum)))
c.new.list[[count]] <- ctemp.new
Sys.sleep(0.1)
setTxtProgressBar(pb, count)
}
}
modelweights
source('priorPARAMETERS.R')
Y.new  <- Y.input
N.new <- nrow(Y.new)
c.new.list <- list(0)
## The number of posterior samples
print("GOING THROUGH MCMC Samples")
pb <- txtProgressBar(min = 1, max = Nps , style = 3)
ctemp.new <- c(0)
cind <- c(0)
modelweights <- c(0)
count = 1
ctemp <- c.list[[count]]
mu <- mu.list[[count]]
S <- S.list[[count]]
g <- table(factor(ctemp, levels = 1:K))
activeclass <- which(g!=0)
## The table function helps converting the data point specific indicator variables to class specific indicator variables
kminus <- length(activeclass)
# active <- activeclass
#Two Auxilary Variables
#The name of the auxilary variables are taken to be one and two more than the maximum value in the already active cluster set
activeclass <- append(activeclass, max(activeclass)+1)
activeclass <- append(activeclass, max(activeclass)+1)
active <- activeclass
### Assigning values to parameters
priortwo <- NA
priortwo <- priordraw(beta, W, epsilon, ro, r, si,N,D, sig2.dat)
mu[active[kminus+1],1:D]  <- priortwo$mu
S[active[kminus+1],1:D,1:D]  <- priortwo$Sigma[1:D,1:D]
priorthree <- NA
priorthree <- priordraw(beta, W, epsilon, ro, r, si,N,D, sig2.dat)
mu[active[kminus+2],1:D]  <- priorthree$mu
S[active[kminus+2],1:D,1:D]  <- priorthree$Sigma[1:D,1:D]
###### Some quantities used to store probabilities
posteriorweight <- matrix(0, nrow = length(active), ncol = N.new)
weights <- matrix(0, nrow = length(active), ncol = N.new)
c.final
for(l in 1:N.new)  {
## Calculating the Expectations and also the normalization constant for the Expectation
for (j in 1:kminus) {
posteriorweight[j,l] <- log(g[active[j]]/ (N-1+alpha))  +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[j],1:D], Q = S[active[j],1:D,1:D], log =TRUE)
}
res <- try(dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+1],1:D], Q= S[active[kminus+1],1:D,1:D]), silent=TRUE)
if (class(res) == "try-error"){
posteriorweight[kminus+1,l] <- -Inf
} else{
posteriorweight[kminus+1,l] <-  log(alpha/ (N-1+alpha)) +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+1],1:D], Q= S[active[kminus+1],1:D,1:D], log = TRUE)
}
res2 <- try(dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+2],1:D], Q= S[active[kminus+2],1:D,1:D]), silent=TRUE)
if (class(res) == "try-error"){
posteriorweight[kminus+2,l] <- -Inf
} else{
posteriorweight[kminus+2,l] <-  log(alpha/ (N-1+alpha))  +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+2],1:D], Q= S[active[kminus+2],1:D,1:D], log = TRUE)
}
weights[,l] <- exp(posteriorweight[,l])/sum(exp(posteriorweight[,l]))
if (sum(exp(posteriorweight[,l])) < 1e-200){
ctemp.new[l] <- sample(active, 1, prob = rep(1,length(active)), replace = TRUE)
} else {
ctemp.new[l] <- sample(active, 1, prob= weights[,l], replace = TRUE)
}
}
weights
c.true
c.final
weights[,9]
weights[,1]
weights[1:N.new,]
weights[2,1:N.new]
which(c.final ==2)
sum(exp(posteriorweight[,l])
)
posteriorweight
sample(active, 1, prob= weights[,l], replace = TRUE)
l
dim(posteriorweight)
apply(posteriorweight,1,sum)
source('priorPARAMETERS.R')
Y.new  <- Y.input
N.new <- nrow(Y.new)
c.new.list <- list(0)
## The number of posterior samples
print("GOING THROUGH MCMC Samples")
pb <- txtProgressBar(min = 1, max = Nps , style = 3)
ctemp.new <- c(0)
cind <- c(0)
modelweights <- c(0)
Y.new <- Y
N.new <- nrow(Y.new)
c.new.list <- list(0)
## The number of posterior samples
print("GOING THROUGH MCMC Samples")
pb <- txtProgressBar(min = 1, max = Nps , style = 3)
ctemp.new <- c(0)
cind <- c(0)
modelweights <- c(0)
count =1
## Assign the parameters to the posterior sample
ctemp <- c.list[[count]]
mu <- mu.list[[count]]
S <- S.list[[count]]
g <- table(factor(ctemp, levels = 1:K))
activeclass <- which(g!=0)
## The table function helps converting the data point specific indicator variables to class specific indicator variables
kminus <- length(activeclass)
# active <- activeclass
#Two Auxilary Variables
#The name of the auxilary variables are taken to be one and two more than the maximum value in the already active cluster set
activeclass <- append(activeclass, max(activeclass)+1)
activeclass <- append(activeclass, max(activeclass)+1)
active <- activeclass
### Assigning values to parameters
priortwo <- NA
priortwo <- priordraw(beta, W, epsilon, ro, r, si,N,D, sig2.dat)
mu[active[kminus+1],1:D]  <- priortwo$mu
S[active[kminus+1],1:D,1:D]  <- priortwo$Sigma[1:D,1:D]
priorthree <- NA
priorthree <- priordraw(beta, W, epsilon, ro, r, si,N,D, sig2.dat)
mu[active[kminus+2],1:D]  <- priorthree$mu
S[active[kminus+2],1:D,1:D]  <- priorthree$Sigma[1:D,1:D]
###### Some quantities used to store probabilities
posteriorweight <- matrix(0, nrow = length(active), ncol = N.new)
weights <- matrix(0, nrow = length(active), ncol = N.new)
weights.final <- c(0)
for(l in 1:N.new)  {
## Calculating the Expectations and also the normalization constant for the Expectation
for (j in 1:kminus) {
posteriorweight[j,l] <- log(g[active[j]]/ (N-1+alpha))  +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[j],1:D], Q = S[active[j],1:D,1:D], log =TRUE)
}
res <- try(dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+1],1:D], Q= S[active[kminus+1],1:D,1:D]), silent=TRUE)
if (class(res) == "try-error"){
posteriorweight[kminus+1,l] <- -Inf
} else{
posteriorweight[kminus+1,l] <-  log(alpha/ (N-1+alpha)) +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+1],1:D], Q= S[active[kminus+1],1:D,1:D], log = TRUE)
}
res2 <- try(dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+2],1:D], Q= S[active[kminus+2],1:D,1:D]), silent=TRUE)
if (class(res) == "try-error"){
posteriorweight[kminus+2,l] <- -Inf
} else{
posteriorweight[kminus+2,l] <-  log(alpha/ (N-1+alpha))  +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+2],1:D], Q= S[active[kminus+2],1:D,1:D], log = TRUE)
}
weights[,l] <- exp(posteriorweight[,l])/sum(exp(posteriorweight[,l]))
if (sum(exp(posteriorweight[,l])) < 1e-200){
ctemp.new[l] <- sample(active, 1, prob = rep(1,length(active)), replace = TRUE)
} else {
ctemp.new[l] <- sample(active, 1, prob= weights[,l], replace = TRUE)
}
weights.final[l] <- posteriorweight[ctemp.new[l],l]
}
weights.final
sum(weights.final)
exp(sum(weights.final)
)
exp(sum(weights.final))
source('priorPARAMETERS.R')
Y.new  <- Y.input
N.new <- nrow(Y.new)
c.new.list <- list(0)
## The number of posterior samples
print("GOING THROUGH MCMC Samples")
pb <- txtProgressBar(min = 1, max = Nps , style = 3)
ctemp.new <- c(0)
modelweights <- c(0)
Y.input <- Y
source('priorPARAMETERS.R')
Y.new  <- Y.input
N.new <- nrow(Y.new)
c.new.list <- list(0)
## The number of posterior samples
print("GOING THROUGH MCMC Samples")
pb <- txtProgressBar(min = 1, max = Nps , style = 3)
ctemp.new <- c(0)
modelweights <- c(0)
for (count in 1:Nps){
## Assign the parameters to the posterior sample
ctemp <- c.list[[count]]
mu <- mu.list[[count]]
S <- S.list[[count]]
g <- table(factor(ctemp, levels = 1:K))
activeclass <- which(g!=0)
## The table function helps converting the data point specific indicator variables to class specific indicator variables
kminus <- length(activeclass)
# active <- activeclass
#Two Auxilary Variables
#The name of the auxilary variables are taken to be one and two more than the maximum value in the already active cluster set
activeclass <- append(activeclass, max(activeclass)+1)
activeclass <- append(activeclass, max(activeclass)+1)
active <- activeclass
### Assigning values to parameters
priortwo <- NA
priortwo <- priordraw(beta, W, epsilon, ro, r, si,N,D, sig2.dat)
mu[active[kminus+1],1:D]  <- priortwo$mu
S[active[kminus+1],1:D,1:D]  <- priortwo$Sigma[1:D,1:D]
priorthree <- NA
priorthree <- priordraw(beta, W, epsilon, ro, r, si,N,D, sig2.dat)
mu[active[kminus+2],1:D]  <- priorthree$mu
S[active[kminus+2],1:D,1:D]  <- priorthree$Sigma[1:D,1:D]
###### Some quantities used to store probabilities
posteriorweight <- matrix(0, nrow = length(active), ncol = N.new)
weights <- matrix(0, nrow = length(active), ncol = N.new)
weights.final <- c(0)
ctemp.new <- c(0)
## This can't be parallelized !!!!!
for(l in 1:N.new)  {
## Calculating the Expectations and also the normalization constant for the Expectation
for (j in 1:kminus) {
posteriorweight[j,l] <- log(g[active[j]]/ (N-1+alpha))  +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[j],1:D], Q = S[active[j],1:D,1:D], log =TRUE)
}
res <- try(dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+1],1:D], Q= S[active[kminus+1],1:D,1:D]), silent=TRUE)
if (class(res) == "try-error"){
posteriorweight[kminus+1,l] <- -Inf
} else{
posteriorweight[kminus+1,l] <-  log(alpha/ (N-1+alpha)) +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+1],1:D], Q= S[active[kminus+1],1:D,1:D], log = TRUE)
}
res2 <- try(dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+2],1:D], Q= S[active[kminus+2],1:D,1:D]), silent=TRUE)
if (class(res) == "try-error"){
posteriorweight[kminus+2,l] <- -Inf
} else{
posteriorweight[kminus+2,l] <-  log(alpha/ (N-1+alpha))  +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+2],1:D], Q= S[active[kminus+2],1:D,1:D], log = TRUE)
}
weights[,l] <- exp(posteriorweight[,l])/sum(exp(posteriorweight[,l]))
if (sum(exp(posteriorweight[,l])) < 1e-200){
ctemp.new[l] <- sample(active, 1, prob = rep(1,length(active)), replace = TRUE)
} else {
ctemp.new[l] <- sample(active, 1, prob= weights[,l], replace = TRUE)
}
weights.final[l] <- posteriorweight[ctemp.new[l],l]
}
modelweights[count] <- sum(weights.final)
c.new.list[[count]] <- ctemp.new
Sys.sleep(0.1)
setTxtProgressBar(pb, count)
}
model.weights
modelweights
Y.new <- relev$Y.test
time.new <- log(relev$time.test)
censoring.new <- relev$censoring.test
c.true.new <- relev$c.true.new
N.new <- nrow(Y.new)
c.new.list <- list(0)
## The number of posterior samples
print("GOING THROUGH MCMC Samples")
pb <- txtProgressBar(min = 1, max = Nps , style = 3)
ctemp.new <- c(0)
modelweights <- c(0)
for (count in 1:Nps){
## Assign the parameters to the posterior sample
ctemp <- c.list[[count]]
mu <- mu.list[[count]]
S <- S.list[[count]]
g <- table(factor(ctemp, levels = 1:K))
activeclass <- which(g!=0)
## The table function helps converting the data point specific indicator variables to class specific indicator variables
kminus <- length(activeclass)
# active <- activeclass
#Two Auxilary Variables
#The name of the auxilary variables are taken to be one and two more than the maximum value in the already active cluster set
activeclass <- append(activeclass, max(activeclass)+1)
activeclass <- append(activeclass, max(activeclass)+1)
active <- activeclass
### Assigning values to parameters
priortwo <- NA
priortwo <- priordraw(beta, W, epsilon, ro, r, si,N,D, sig2.dat)
mu[active[kminus+1],1:D]  <- priortwo$mu
S[active[kminus+1],1:D,1:D]  <- priortwo$Sigma[1:D,1:D]
priorthree <- NA
priorthree <- priordraw(beta, W, epsilon, ro, r, si,N,D, sig2.dat)
mu[active[kminus+2],1:D]  <- priorthree$mu
S[active[kminus+2],1:D,1:D]  <- priorthree$Sigma[1:D,1:D]
###### Some quantities used to store probabilities
posteriorweight <- matrix(0, nrow = length(active), ncol = N.new)
weights <- matrix(0, nrow = length(active), ncol = N.new)
weights.final <- c(0)
ctemp.new <- c(0)
## This can't be parallelized !!!!!
for(l in 1:N.new)  {
## Calculating the Expectations and also the normalization constant for the Expectation
for (j in 1:kminus) {
posteriorweight[j,l] <- log(g[active[j]]/ (N-1+alpha))  +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[j],1:D], Q = S[active[j],1:D,1:D], log =TRUE)
}
res <- try(dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+1],1:D], Q= S[active[kminus+1],1:D,1:D]), silent=TRUE)
if (class(res) == "try-error"){
posteriorweight[kminus+1,l] <- -Inf
} else{
posteriorweight[kminus+1,l] <-  log(alpha/ (N-1+alpha)) +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+1],1:D], Q= S[active[kminus+1],1:D,1:D], log = TRUE)
}
res2 <- try(dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+2],1:D], Q= S[active[kminus+2],1:D,1:D]), silent=TRUE)
if (class(res) == "try-error"){
posteriorweight[kminus+2,l] <- -Inf
} else{
posteriorweight[kminus+2,l] <-  log(alpha/ (N-1+alpha))  +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+2],1:D], Q= S[active[kminus+2],1:D,1:D], log = TRUE)
}
weights[,l] <- exp(posteriorweight[,l])/sum(exp(posteriorweight[,l]))
if (sum(exp(posteriorweight[,l])) < 1e-200){
ctemp.new[l] <- sample(active, 1, prob = rep(1,length(active)), replace = TRUE)
} else {
ctemp.new[l] <- sample(active, 1, prob= weights[,l], replace = TRUE)
}
weights.final[l] <- posteriorweight[ctemp.new[l],l]
}
modelweights[count] <- sum(weights.final)
c.new.list[[count]] <- ctemp.new
Sys.sleep(0.1)
setTxtProgressBar(pb, count)
}
modelweights
c.matrix.new <- matrix(NA, nrow = N.new, ncol = Nps)
for( h in 1:Nps){
c.matrix.new[,h] <- c.new.list[[h]]
}
c.matrix.new <<- c.matrix.new
c.matrix.new
psm2 <- comp.psm(t(c.matrix.new))
mpear2 <- maxpear(psm2)
mpear2$cl
c.sbc.new <- c(0)
## As the clusters are different we switch the labels
for ( i in 1:N.test){
if(mpear2$cl[i] ==1){
c.sbc.new[i] = 1
} else {
c.sbc.new[i] = 2
}}
for ( i in 1:N.new){
if(mpear2$cl[i] ==1){
c.sbc.new[i] = 1
} else {
c.sbc.new[i] = 2
}}
c.sbc.new
c.sbc
F
pre.sbc <- c(0)
for ( q in 1:F){
ind <- which(c.sbc == q)
ind.new <- which(c.sbc.new == q)
time.tmp <- time[ind]
Y.tmp <- Y[ind,]
Y.tmp.new <- Y.new[ind.new,]
reg <- cv.glmnet(x = Y.tmp, y = time.tmp, family = "gaussian")
pre.sbc[ind.new] <- predict(object = reg, newx = Y.tmp.new, s = "lambda.min")
}
as.numeric(survConcordance(smod.new ~ exp(-pre.sbc))[1])
dim(Y)
dim(Y.new)
pc <- prcomp(Y.new)
pc.pred <- predict(pc,newdata = Y.new)
p1 <- ggplot(as.data.frame(pc.pred), aes(x=pc.pred[,1], y= pc.pred[,2], colour= as.factor(c.sbc.new))) + ggtitle(" SBC Clustering \n Breast Cancer Test Set") + geom_point(shape=19) + labs(y = "PC1", x = "PC2", colour = "Classes")
p1
surv.fit <- survfit(smod.new ~ c.sbc.new)
surv.fit
logrank <- survdiff(smod.new ~ c.sbc.new)
logrank
lr <- c(0)
for (j in 1:Nps){
lr[j] <-  1 - pchisq(unlist(survdiff(smod.new ~ c.new.list[[j]]))$chisq,df =1)
}
lr
which.min(lr)
lr[which.min(lr)]
modelweights
cbind(lr,modelweights)
dim(Y.input)
dim(Y)
dim(Y.new)
N.new
i =1
survdiff(smod.new ~ c.new.list[[j]])
j
j =1
survdiff(smod.new ~ c.new.list[[j]])
cov((1-lr), modelweights, method = "pearson")
cor((1-lr), modelweights, method = "pearson")
modelweights
Y.input <- Y
source('priorPARAMETERS.R')
Y.new  <- Y.input
N.new <- nrow(Y.new)
c.new.list <- list(0)
## The number of posterior samples
print("GOING THROUGH MCMC Samples")
pb <- txtProgressBar(min = 1, max = Nps , style = 3)
ctemp.new <- c(0)
modelweights <- c(0)
for (count in 1:Nps){
## Assign the parameters to the posterior sample
ctemp <- c.list[[count]]
mu <- mu.list[[count]]
S <- S.list[[count]]
g <- table(factor(ctemp, levels = 1:K))
activeclass <- which(g!=0)
## The table function helps converting the data point specific indicator variables to class specific indicator variables
kminus <- length(activeclass)
# active <- activeclass
#Two Auxilary Variables
#The name of the auxilary variables are taken to be one and two more than the maximum value in the already active cluster set
activeclass <- append(activeclass, max(activeclass)+1)
activeclass <- append(activeclass, max(activeclass)+1)
active <- activeclass
### Assigning values to parameters
priortwo <- NA
priortwo <- priordraw(beta, W, epsilon, ro, r, si,N,D, sig2.dat)
mu[active[kminus+1],1:D]  <- priortwo$mu
S[active[kminus+1],1:D,1:D]  <- priortwo$Sigma[1:D,1:D]
priorthree <- NA
priorthree <- priordraw(beta, W, epsilon, ro, r, si,N,D, sig2.dat)
mu[active[kminus+2],1:D]  <- priorthree$mu
S[active[kminus+2],1:D,1:D]  <- priorthree$Sigma[1:D,1:D]
###### Some quantities used to store probabilities
posteriorweight <- matrix(0, nrow = length(active), ncol = N.new)
weights <- matrix(0, nrow = length(active), ncol = N.new)
weights.final <- c(0)
ctemp.new <- c(0)
## This can't be parallelized !!!!!
for(l in 1:N.new)  {
## Calculating the Expectations and also the normalization constant for the Expectation
for (j in 1:kminus) {
posteriorweight[j,l] <- log(g[active[j]]/ (N-1+alpha))  +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[j],1:D], Q = S[active[j],1:D,1:D], log =TRUE)
}
res <- try(dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+1],1:D], Q= S[active[kminus+1],1:D,1:D]), silent=TRUE)
if (class(res) == "try-error"){
posteriorweight[kminus+1,l] <- -Inf
} else{
posteriorweight[kminus+1,l] <-  log(alpha/ (N-1+alpha)) +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+1],1:D], Q= S[active[kminus+1],1:D,1:D], log = TRUE)
}
res2 <- try(dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+2],1:D], Q= S[active[kminus+2],1:D,1:D]), silent=TRUE)
if (class(res) == "try-error"){
posteriorweight[kminus+2,l] <- -Inf
} else{
posteriorweight[kminus+2,l] <-  log(alpha/ (N-1+alpha))  +  dMVN(as.vector(t(Y.new[l,1:D])), mean = mu[active[kminus+2],1:D], Q= S[active[kminus+2],1:D,1:D], log = TRUE)
}
weights[,l] <- exp(posteriorweight[,l])/sum(exp(posteriorweight[,l]))
if (sum(exp(posteriorweight[,l])) < 1e-200){
ctemp.new[l] <- sample(active, 1, prob = rep(1,length(active)), replace = TRUE)
} else {
ctemp.new[l] <- sample(active, 1, prob= weights[,l], replace = TRUE)
}
weights.final[l] <- posteriorweight[ctemp.new[l],l]
}
modelweights[count] <- sum(weights.final)
c.new.list[[count]] <- ctemp.new
Sys.sleep(0.1)
setTxtProgressBar(pb, count)
}
## Converting the list to a matrix
c.matrix.new <- matrix(NA, nrow = N.new, ncol = Nps)
for( h in 1:Nps){
c.matrix.new[,h] <- c.new.list[[h]]
}
c.matrix.new <<- c.matrix.new
#### Choose that configuration which has the highest difference in survival curves
lr <- c(0)
for (j in 1:Nps){
lr[j] <-  1 - pchisq(unlist(survdiff(smod.new ~ c.new.list[[j]]))$chisq,df =1)
}
lr <- c(0)
for (j in 1:Nps){
lr[j] <-  1 - pchisq(unlist(survdiff(smod ~ c.new.list[[j]]))$chisq,df =1)
}
cor((1-lr), modelweights, method = "pearson")
modelweights
lr
table(as.factor(lr))
numclust <- table(factor(c, levels = 1:K))
activeclass<- which(numclust!=0)
activeclass
Ytemp <- matrix(NA, nrow = N, ncol = D)
ctemp <- c
time
cbind(time, That)
disclass <- table(factor(c, levels = 1:K))
activeclass <- which(disclass!=0)
Ytemp <- Y
Ytemp.scaled <- matrix(NA, nrow = N, ncol = D)
for ( i in 1:length(activeclass)) {
clust <- which(c == activeclass[i])
if (length(clust)==1){
Ytemp.scaled[clust,1:D] <- matrix(0, nrow =1, ncol =D)
} else {
Ytemp.scaled[clust,1:D] <- scale(Ytemp[clust,1:D], center = TRUE, scale = TRUE)
}
}
loglikelihood <-c(0)
activeclass
j =1
loglikelihood[j] <- 0
clust <- which(c == activeclass[j])
luk1 <- c(0)
luk2 <- c(0)
clust
length(clust)
